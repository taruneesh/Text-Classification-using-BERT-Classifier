{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HQ0zi0HlIxgZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRFVbNc3aAV9",
    "outputId": "37de0abe-ed58-4f0c-df1c-a800c9175a43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tarun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQ4lehqtaKru",
    "outputId": "320eac1c-b203-48ee-9d13-0f44b1c80f21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tarun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "oSZak8JdN0fp",
    "outputId": "aecd168d-21a2-472b-ee27-eb27d83352c7"
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('stackexchange_data.csv')\n",
    "# def clean_text(Data):\n",
    "#     all_questions = list()\n",
    "#     lines= Data['question_text'].values.tolist()\n",
    "#     for text in lines:\n",
    "#         text = text.lower()\n",
    "#         text= re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\",\"\",text)\n",
    "#         Data['tokenized_sents'] = Data.apply(lambda row: nltk.word_tokenize(row['question_text']), axis=1)\n",
    "#         table = str.maketrans('','', string.punctuation)\n",
    "        \n",
    "        \n",
    "#         Data['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "#     return all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l7fahei6q5y8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source</th>\n",
       "      <th>votes_count</th>\n",
       "      <th>answers_count</th>\n",
       "      <th>status</th>\n",
       "      <th>views_count</th>\n",
       "      <th>date</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_reputation_score</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_link</th>\n",
       "      <th>tags</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stackexchange</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unanswered</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-10 20:40:04Z</td>\n",
       "      <td>StuckonEngagement</td>\n",
       "      <td>38607.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28140</td>\n",
       "      <td>Use task assignments as resource engagements</td>\n",
       "      <td>/questions/28140/use-task-assignments-as-resou...</td>\n",
       "      <td>ms-project %</td>\n",
       "      <td>In Microsoft Professional, is there a way for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>stackexchange</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>answered</td>\n",
       "      <td>514</td>\n",
       "      <td>2020-02-09 21:23:49Z</td>\n",
       "      <td>Ariser</td>\n",
       "      <td>31614.0</td>\n",
       "      <td>141</td>\n",
       "      <td>28133</td>\n",
       "      <td>How to formulate risks well for risk analysis?</td>\n",
       "      <td>/questions/28133/how-to-formulate-risks-well-f...</td>\n",
       "      <td>risk-management %</td>\n",
       "      <td>If I have to formulate goals for a project the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>stackexchange</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>answered</td>\n",
       "      <td>974</td>\n",
       "      <td>2020-02-07 13:41:12Z</td>\n",
       "      <td>bitshift</td>\n",
       "      <td>34111.0</td>\n",
       "      <td>171</td>\n",
       "      <td>28125</td>\n",
       "      <td>Does it make sense to use Scrum when the dates...</td>\n",
       "      <td>/questions/28125/does-it-make-sense-to-use-scr...</td>\n",
       "      <td>scrum % sprint %</td>\n",
       "      <td>Does it make sense to use Scrum when the dates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>stackexchange</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>answered</td>\n",
       "      <td>21</td>\n",
       "      <td>2020-02-06 16:46:45Z</td>\n",
       "      <td>Aldie</td>\n",
       "      <td>38571.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28123</td>\n",
       "      <td>MS project shows a PV of 0 for task with resou...</td>\n",
       "      <td>/questions/28123/ms-project-shows-a-pv-of-0-fo...</td>\n",
       "      <td>ms-project %</td>\n",
       "      <td>I am using MS Project to calculate the Earn Va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>stackexchange</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unanswered</td>\n",
       "      <td>26</td>\n",
       "      <td>2020-02-06 14:09:32Z</td>\n",
       "      <td>Il Siva</td>\n",
       "      <td>38570.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28122</td>\n",
       "      <td>Business Analyst and Product Release</td>\n",
       "      <td>/questions/28122/business-analyst-and-product-...</td>\n",
       "      <td>scrum % agile % business-analyst %</td>\n",
       "      <td>What are the responsibilities of a business an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         source  votes_count  answers_count      status views_count  \\\n",
       "0      0  stackexchange            0              0  unanswered           5   \n",
       "1      1  stackexchange            4              3    answered         514   \n",
       "2      2  stackexchange            7              4    answered         974   \n",
       "3      3  stackexchange            0              1    answered          21   \n",
       "4      4  stackexchange            0              0  unanswered          26   \n",
       "\n",
       "                   date          user_name  user_id user_reputation_score  \\\n",
       "0  2020-02-10 20:40:04Z  StuckonEngagement  38607.0                     1   \n",
       "1  2020-02-09 21:23:49Z             Ariser  31614.0                   141   \n",
       "2  2020-02-07 13:41:12Z           bitshift  34111.0                   171   \n",
       "3  2020-02-06 16:46:45Z              Aldie  38571.0                     1   \n",
       "4  2020-02-06 14:09:32Z            Il Siva  38570.0                     1   \n",
       "\n",
       "   question_id                                     question_title  \\\n",
       "0        28140       Use task assignments as resource engagements   \n",
       "1        28133     How to formulate risks well for risk analysis?   \n",
       "2        28125  Does it make sense to use Scrum when the dates...   \n",
       "3        28123  MS project shows a PV of 0 for task with resou...   \n",
       "4        28122              Business Analyst and Product Release    \n",
       "\n",
       "                                       question_link  \\\n",
       "0  /questions/28140/use-task-assignments-as-resou...   \n",
       "1  /questions/28133/how-to-formulate-risks-well-f...   \n",
       "2  /questions/28125/does-it-make-sense-to-use-scr...   \n",
       "3  /questions/28123/ms-project-shows-a-pv-of-0-fo...   \n",
       "4  /questions/28122/business-analyst-and-product-...   \n",
       "\n",
       "                                  tags  \\\n",
       "0                        ms-project %    \n",
       "1                   risk-management %    \n",
       "2                    scrum % sprint %    \n",
       "3                        ms-project %    \n",
       "4  scrum % agile % business-analyst %    \n",
       "\n",
       "                                       question_text  \n",
       "0  In Microsoft Professional, is there a way for ...  \n",
       "1  If I have to formulate goals for a project the...  \n",
       "2  Does it make sense to use Scrum when the dates...  \n",
       "3  I am using MS Project to calculate the Earn Va...  \n",
       "4  What are the responsibilities of a business an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[\"new_column\"] = Data['question_text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['tokenized_sents'] = Data.apply(lambda row: nltk.word_tokenize(row['new_column']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [In, Microsoft, Professional, is, there, a, wa...\n",
       "1       [If, I, have, to, formulate, goals, for, a, pr...\n",
       "2       [Does, it, make, sense, to, use, Scrum, when, ...\n",
       "3       [I, am, using, MS, Project, to, calculate, the...\n",
       "4       [What, are, the, responsibilities, of, a, busi...\n",
       "                              ...                        \n",
       "5330    [What, is, the, difference, between, Vision, a...\n",
       "5331    [Ive, had, trouble, in, the, past, explaining,...\n",
       "5332    [I, have, the, following, situation, in, my, o...\n",
       "5333    [From, reading, the, entry, on, Wikipedia, Cap...\n",
       "5334    [I, am, curious, to, the, information, and, le...\n",
       "Name: tokenized_sents, Length: 5335, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['tokenized_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [In, Microsoft, Professional, way, resource, t...\n",
       "1       [If, I, formulate, goals, project, theres, wel...\n",
       "2       [Does, make, sense, use, Scrum, dates, sprints...\n",
       "3       [I, using, MS, Project, calculate, Earn, Value...\n",
       "4       [What, responsibilities, business, analyst, pr...\n",
       "                              ...                        \n",
       "5330    [What, difference, Vision, Scope, Document, Pr...\n",
       "5331    [Ive, trouble, past, explaining, project, risk...\n",
       "5332    [I, following, situation, organization, Some, ...\n",
       "5333    [From, reading, entry, Wikipedia, Capability, ...\n",
       "5334    [I, curious, information, level, detail, peopl...\n",
       "Name: tokenized_sents, Length: 5335, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "Data['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [in, microsoft, profession, is, there, a, way,...\n",
       "1       [if, i, have, to, formul, goal, for, a, projec...\n",
       "2       [doe, it, make, sens, to, use, scrum, when, th...\n",
       "3       [i, am, use, ms, project, to, calcul, the, ear...\n",
       "4       [what, are, the, respons, of, a, busi, analyst...\n",
       "                              ...                        \n",
       "5330    [what, is, the, differ, between, vision, and, ...\n",
       "5331    [ive, had, troubl, in, the, past, explain, pro...\n",
       "5332    [i, have, the, follow, situat, in, my, organ, ...\n",
       "5333    [from, read, the, entri, on, wikipedia, capabl...\n",
       "5334    [i, am, curious, to, the, inform, and, level, ...\n",
       "Name: stemmed, Length: 5335, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['stemmed'] = Data['tokenized_sents'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "Data = Data.drop(columns=['tokenized_sents']) # Get rid of the unstemmed column.\n",
    "Data['stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(Data):\n",
    "#     all_questions = list()\n",
    "#     lines= Data['question_text'].values.tolist()\n",
    "#     for text in lines:\n",
    "#         text = text.lower()\n",
    "#         text= re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\",\"\",text)\n",
    "#         Data['tokenized_sents'] = Data.apply(lambda row: nltk.word_tokenize(row['question_text']), axis=1)\n",
    "#         table = str.maketrans('','', string.punctuation)\n",
    "        \n",
    "        \n",
    "#         Data['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "#     return all_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = Data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-527340688e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Encode the input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m# The transformers model expects the target class column to be named \"labels\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename_column_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# def encode_batch(batch):\n",
    "#     return tokenizer(\n",
    "#       batch[\"premise\"],\n",
    "#       batch[\"hypothesis\"],\n",
    "#       max_length=180,\n",
    "#       truncation=True,\n",
    "#       padding=\"max_length\"\n",
    "#   )\n",
    "\n",
    "# # Encode the input data\n",
    "# Data = Data.map(encode_batch, batched=True)\n",
    "# # The transformers model expects the target class column to be named \"labels\"\n",
    "# Data.rename_column_(\"label\", \"labels\")\n",
    "# # Transform to pytorch tensors and only output the required columns\n",
    "# Data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'bert' has no attribute 'bert_tokenization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5f627511cf80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mBertTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_tokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n\u001b[1;32m      3\u001b[0m                             trainable=False)\n\u001b[1;32m      4\u001b[0m \u001b[0mvocabulary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mto_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'bert' has no attribute 'bert_tokenization'"
     ]
    }
   ],
   "source": [
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Software enginnering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
